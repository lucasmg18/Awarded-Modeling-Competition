{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# MODELO DCGAN ----- PROBLEMA DE MODELIZACIÓN DE EMPRESAS GMV\n","\n","\n","El modelo de DCGAN que se muestra a continuación se trata de una modificación del que construyó Radford en el paper que se muestra a continuación. En el apartado del generador se expondrán las diferencias que presenta nuestro modelo con el suyo, a nivel arquitectónico. El paper en cuestión se puede encontrar en el siguiente link: [Paper original](https://arxiv.org/abs/1511.06434).\n","\n","En lo que sigue, se procede a mostrar la implementación en python de este modelo y su aplicación a un conjunto de imágenes que habían sido previamente tratadas mediante modelos de Data Augmentation."],"metadata":{"id":"Pj4f2WKXzojc"}},{"cell_type":"markdown","source":["### IMPORTACIÓN DE ALGUNAS DE LAS LIBRERÍAS NECESARIAS"],"metadata":{"id":"hCAiVTEGzojm"}},{"cell_type":"code","source":["import os\n","import time\n","import tensorflow as tf\n","#import tensorflow.compat.v1 as tf----- Estas dos lineas solo si se quiera ejecutar en un entorno que no tenga tf1. Poco recomendable, tarda mucho más.\n","                                        #Preferible hacerlo en un entorno que reconozca tf1, como kaggle, por ejemplo.\n","#tf.disable_v2_behavior()\n","import numpy as np\n","from glob import glob\n","import datetime\n","import random\n","import PIL\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook as tqdm\n","%matplotlib inline\n","import urllib\n","import tarfile\n","import xml.etree.ElementTree as ET\n","from imageio import imread, imsave, mimsave\n","import shutil\n","import cv2\n","import glob\n","from imageio import imread, imsave, mimsave"],"metadata":{"_kg_hide-input":true,"_uuid":"267d5504009eca2b809a2691131366baebe98436","execution":{"iopub.status.busy":"2023-11-03T15:06:58.784606Z","iopub.execute_input":"2023-11-03T15:06:58.784941Z","iopub.status.idle":"2023-11-03T15:07:00.324540Z","shell.execute_reply.started":"2023-11-03T15:06:58.784884Z","shell.execute_reply":"2023-11-03T15:07:00.323627Z"},"trusted":true,"id":"0G5p3gS2zojn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CODIFICACIÓN DE LOS ELEMENTOS QUE CONFORMAN LA ARQUITECTURA DE LA RED"],"metadata":{"id":"_cVCYPr0zojq"}},{"cell_type":"markdown","source":["### Generador"],"metadata":{"id":"UI8ceqa_zojq"}},{"cell_type":"markdown","source":["### VAMOS A COMENTAR LAS FUNCIONES QUE HEMOS UTILIZADO PARA CREAR EL GENERERADOR:\n","\n","**tf.variable_scope()**: Crea un entorno que permite administrar variables. Sus argumentos:\n","\n","    -'generator': El nombre del entorno\n","    \n","    - reuse=not training: Decide si las variables del entorno se reutilzarán o crearán de nuevo. Su valor es el contrario del valor booleano training, un argumento del generador.\n","\n","**tf.layers.dense()**: Crea una capa completamente conectada. Sus argumentos:\n","\n","    - z: El argumento de entrada\n","    \n","    - 4*4*512: Número de unidades de la capa\n","    \n","**tf.reshape()**: Prepara la capa para ser introducida en la primera capa convolucional. Sus argumentos:\n","\n","    - fully_connected: Denota la capa que se quiere modelar\n","    \n","    -(-1, 4, 4, 512): Modfifica la dimensionalidad del tensor para adaptarlo a la capa convolucional.\n","    \n","    -1: Se utiliza para qur tf infiera esta dimensión para que el conjunto de elementos del tensor se mantenga constante.\n","    \n","**tf.nn.leaky_relu()**: La función de activación de la capa. Sus argumentos:\n","\n","    -fully_connected: El sujeto de la activación.\n","    \n","    -Leaky Relu: La función de activación\n","    \n","**tf.layers.conv2d_transpose()**: Crea una capa de deconvolución. Sus argumentos:\n","\n","    -fully_connected: El tensor de entrada al que se le va a aplicar la capa convolucional.\n","    \n","    -filtros: Especifica el número de canales que va a tener la capa. A lo largo de todo el generador va a denotar la myor dimensión de la siguiente capa convolucional, p.e.: para la primera 256.\n","    \n","    - kernel_size: Denota el tamaño del kernel que se va a aplicar en la operación convolución. Se mantendrá en [5,5] en todo el problema.\n","    \n","    - strides: Denota cuánto se desplaza el kernel en cada convolución. Se mantendrá constante en [2,2], salvo en la capa de salida, que será [1,1].\n","    \n","    - padding=\"SAME\": Indica la estrategia del borde de relleno. Usamos SAME para garantizar la dimensionalidad del tensor.\n","    \n","    - kernel_initializer: Indica la inicilización de los pesos en la capa convolucional. Este valor depende de una variable global predeefinida.\n","    \n","    - name=\"trans_conv1\": Nombre de la capa\n","\n","**tf.layers.batch_normalization()**: Aplica una capa de normalización por lotes. En el pdf se puede encontrar por qué es beneficioso utilizar este filtro. Sus argumentos:\n","\n","    - input: la capa a ser normalizada.\n","    \n","    - training: Valor booleano que dicta el comportamiento de la normalización por lotes: si training es True, entonces se utiliza la estadística del lote actual, mientras que si es False, se utilizan las de lotes pasados.\n","    \n","    - epsilon=EPSILON: Valor (pequeño) que se suma a la varianza del lote. Depende de una variable global predefinida.\n","    \n","    - name: El nombre\n","\n","Por último, para la función de activación final (que rige el ouput), hemos decidido utilizar la función tangente hiperbólica usual."],"metadata":{"id":"LntzbhVczojr"}},{"cell_type":"code","source":["\n","def generator(z, output_channel_dim, training):\n","    with tf.variable_scope(\"generator\", reuse= not training):\n","\n","        # 4x4x512\n","        fully_connected = tf.layers.dense(z, 4*4*512)\n","        fully_connected = tf.reshape(fully_connected, (-1, 4, 4, 512))\n","        fully_connected = tf.nn.leaky_relu(fully_connected)\n","\n","        # 4x4x512 -> 8x8x256\n","        trans_conv1 = tf.layers.conv2d_transpose(inputs=fully_connected,\n","                                                 filters=256,\n","                                                 kernel_size=[5,5],\n","                                                 strides=[2,2],\n","                                                 padding=\"SAME\",\n","                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                                 name=\"trans_conv1\")\n","        batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1,\n","                                                          training=training,\n","                                                          epsilon=EPSILON,\n","                                                          name=\"batch_trans_conv1\")\n","        trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1,\n","                                           name=\"trans_conv1_out\")\n","\n","        # 8x8x256 -> 16x16x128\n","        trans_conv2 = tf.layers.conv2d_transpose(inputs=trans_conv1_out,\n","                                                 filters=128,\n","                                                 kernel_size=[5,5],\n","                                                 strides=[2,2],\n","                                                 padding=\"SAME\",\n","                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                                 name=\"trans_conv2\")\n","        batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2,\n","                                                          training=training,\n","                                                          epsilon=EPSILON,\n","                                                          name=\"batch_trans_conv2\")\n","        trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2,\n","                                           name=\"trans_conv2_out\")\n","\n","        # 16x16x128 -> 32x32x64\n","        trans_conv3 = tf.layers.conv2d_transpose(inputs=trans_conv2_out,\n","                                                 filters=64,\n","                                                 kernel_size=[5,5],\n","                                                 strides=[2,2],\n","                                                 padding=\"SAME\",\n","                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                                 name=\"trans_conv3\")\n","        batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3,\n","                                                          training=training,\n","                                                          epsilon=EPSILON,\n","                                                          name=\"batch_trans_conv3\")\n","        trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3,\n","                                           name=\"trans_conv3_out\")\n","\n","\n","        # 32x32x64 -> 64x64x32\n","        trans_conv4 = tf.layers.conv2d_transpose(inputs=trans_conv3_out,\n","                                                 filters=32,\n","                                                 kernel_size=[5,5],\n","                                                 strides=[2,2],\n","                                                 padding=\"SAME\",\n","                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                                 name=\"trans_conv4\")\n","        batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4,\n","                                                          training=training,\n","                                                          epsilon=EPSILON,\n","                                                          name=\"batch_trans_conv4\")\n","        trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4,\n","                                           name=\"trans_conv4_out\")\n","\n","        # 64x64x32 -> 64x64x3\n","        logits = tf.layers.conv2d_transpose(inputs=trans_conv4_out,\n","                                            filters=3,\n","                                            kernel_size=[5,5],\n","                                            strides=[1,1],\n","                                            padding=\"SAME\",\n","                                            kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                            name=\"logits\")\n","        out = tf.tanh(logits, name=\"out\")\n","        return out"],"metadata":{"_uuid":"fc376df46433261bfeb643a95793718a9d969ed1","execution":{"iopub.status.busy":"2023-11-03T15:07:00.326441Z","iopub.execute_input":"2023-11-03T15:07:00.326753Z","iopub.status.idle":"2023-11-03T15:07:00.346684Z","shell.execute_reply.started":"2023-11-03T15:07:00.326693Z","shell.execute_reply":"2023-11-03T15:07:00.345935Z"},"trusted":true,"id":"lglmqdHTzojs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Discriminador"],"metadata":{"id":"0zNtBIudzojt"}},{"cell_type":"markdown","source":["### La red discriminadora sigue los mismos argumentos que la generadora, con la salvedad de que la función de output es una sigmoide. La explicación sobre la arquitectura de ambas redes se puede encontrar en el pdf."],"metadata":{"id":"3ydtblTAzoju"}},{"cell_type":"code","source":["def discriminator(x, reuse):\n","    with tf.variable_scope(\"discriminator\", reuse=reuse):\n","\n","        # 64x64x3 -> 32x32x32\n","        conv1 = tf.layers.conv2d(inputs=x,\n","                                 filters=32,\n","                                 kernel_size=[5,5],\n","                                 strides=[2,2],\n","                                 padding=\"SAME\",\n","                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                 name='conv1')\n","        batch_norm1 = tf.layers.batch_normalization(conv1,\n","                                                    training=True,\n","                                                    epsilon=EPSILON,\n","                                                    name='batch_norm1')\n","        conv1_out = tf.nn.leaky_relu(batch_norm1,\n","                                     name=\"conv1_out\")\n","\n","        # 32x32x32 -> 16x16x64\n","        conv2 = tf.layers.conv2d(inputs=conv1_out,\n","                                 filters=64,\n","                                 kernel_size=[5, 5],\n","                                 strides=[2, 2],\n","                                 padding=\"SAME\",\n","                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                 name='conv2')\n","        batch_norm2 = tf.layers.batch_normalization(conv2,\n","                                                    training=True,\n","                                                    epsilon=EPSILON,\n","                                                    name='batch_norm2')\n","        conv2_out = tf.nn.leaky_relu(batch_norm2,\n","                                     name=\"conv2_out\")\n","\n","        # 16x16x64 -> 8x8x128\n","        conv3 = tf.layers.conv2d(inputs=conv2_out,\n","                                 filters=128,\n","                                 kernel_size=[5, 5],\n","                                 strides=[2, 2],\n","                                 padding=\"SAME\",\n","                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                 name='conv3')\n","        batch_norm3 = tf.layers.batch_normalization(conv3,\n","                                                    training=True,\n","                                                    epsilon=EPSILON,\n","                                                    name='batch_norm3')\n","        conv3_out = tf.nn.leaky_relu(batch_norm3,\n","                                     name=\"conv3_out\")\n","\n","        # 8x8x128 -> 8x8x256\n","        conv4 = tf.layers.conv2d(inputs=conv3_out,\n","                                 filters=256,\n","                                 kernel_size=[5, 5],\n","                                 strides=[1, 1],\n","                                 padding=\"SAME\",\n","                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                 name='conv4')\n","        batch_norm4 = tf.layers.batch_normalization(conv4,\n","                                                    training=True,\n","                                                    epsilon=EPSILON,\n","                                                    name='batch_norm4')\n","        conv4_out = tf.nn.leaky_relu(batch_norm4,\n","                                     name=\"conv4_out\")\n","\n","        # 8x8x256 -> 4x4x512\n","\n","        conv5 = tf.layers.conv2d(inputs=conv4_out,\n","                                filters=512,\n","                                kernel_size=[5, 5],\n","                                strides=[2, 2],\n","                                padding=\"SAME\",\n","                                kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n","                                name='conv5')\n","        batch_norm5 = tf.layers.batch_normalization(conv5,\n","                                                    training=True,\n","                                                    epsilon=EPSILON,\n","                                                    name='batch_norm5')\n","        conv5_out = tf.nn.leaky_relu(batch_norm5,\n","                                     name=\"conv5_out\")\n","\n","        flatten = tf.reshape(conv5_out, (-1, 4*4*512))\n","        logits = tf.layers.dense(inputs=flatten,\n","                                 units=1,\n","                                 activation=None)\n","        out = tf.sigmoid(logits)\n","        return out, logits"],"metadata":{"_uuid":"ba53a4bb09dbcd57d3e3392f74ccd054ecf23ecb","execution":{"iopub.status.busy":"2023-11-03T15:07:00.348054Z","iopub.execute_input":"2023-11-03T15:07:00.348273Z","iopub.status.idle":"2023-11-03T15:07:00.369623Z","shell.execute_reply.started":"2023-11-03T15:07:00.348235Z","shell.execute_reply":"2023-11-03T15:07:00.368838Z"},"trusted":true,"id":"IvUyMi7Azojv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Función de pérdida y optimizador"],"metadata":{"id":"U2smlw5Bzojw"}},{"cell_type":"markdown","source":["### VAMOS A COMENTAR LAS FUNCIONES QUE HEMOS UTILIZADO PARA CREAR LA FUNCIÓN DE PERDIDA:\n","\n","**Variable g_model**: Utilizando el código del generador con los parámetros antes explicados que son input_z (vector de ruido) y el output_channel_dim qeu son los 3 colores del RGB.\n","\n","**Variable noisy_input_real**: Creamos ruido en el input_real a través a través de la siguiente función:\n","\n","    - tf.random_normal: Genera un valor aleatorio de una normal de n-dimensional.\n","    \n","**Variables d_model_real, d_logits_real**: Genera el discriminador con el input_real a través de noisy_input_real.\n","    \n","**Variables d_model_fake, d_logits_fake**: Genera el discriminador con el g_model(modelo generado a traves del generador con el input de ruido)\n","\n","**tf.nn.sigmoid_cross_entropy_with_logits**: alcula la pérdida de entropía cruzada entre las predicciones (logits) y las etiquetas (targets) utilizando la función de activación sigmoide en el proceso. El resultado es un tensor que contiene los valores de pérdida para cada ejemplo en el lote de datos.\n","\n","    -logits: Es un tensor que representa las predicciones antes de pasar por la función de activación sigmoide. Estos valores suelen ser salidas crudas del modelo, que pueden ser positivos o negativos.\n","\n","    -targets: Es un tensor que contiene las etiquetas reales (etiquetas binarias) a comparar con las predicciones. Estas etiquetas son generalmente valores binarios (0 o 1) que indican si un ejemplo pertenece o no a una clase específica.\n","\n","    -name (opcional): Un nombre opcional para la operación.\n","\n","**tf.reduce_mean()**: Hace la media de los elementos de un tensor. Además del parámetro input_tensor tendremos los parámetros opcionales axis,keepdims y name que los mantenermos por defecto\n","\n","    -input_tensor: El tensor de entrada al que se le va a aplicar, que en nuestro caso es el devuelto por la funcion anterior\n","    \n","**Variables d_loss_real y d_loss_fake**: Muestra como de bien el discriminador clasifica las imágenes reales como reales(d_loss_real) y las generadas como generadas(d_loss_fake).\n","\n","**Variable g_loss(pérdida generativa)**: Muestra como de bien el generador esta realizando muestras reales que son difíciles de distinguir para el descriminador.\n","\n","**Variable d_loss(pérdida del discriminador)**: Mide como de bien el discriminador puede distinguir entre los datos reales y los generados.(haciendo la media entre d_loss_real y d_loss_fake).\n","\n","Por último, para la función de activación final (que rige el ouput), hemos decidido utilizar la función tangente hiperbólica usual."],"metadata":{"id":"SVU1DpmNzojw"}},{"cell_type":"code","source":["def model_loss(input_real, input_z, output_channel_dim):\n","    g_model = generator(input_z, output_channel_dim, True)\n","\n","    noisy_input_real = input_real + tf.random_normal(shape=tf.shape(input_real),\n","                                                     mean=0.0,\n","                                                     stddev=random.uniform(0.0, 0.1),\n","                                                     dtype=tf.float32)\n","\n","    d_model_real, d_logits_real = discriminator(noisy_input_real, reuse=False)\n","    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n","\n","    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n","                                                                         labels=tf.ones_like(d_model_real)*random.uniform(0.9, 1.0)))\n","    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n","                                                                         labels=tf.zeros_like(d_model_fake)))\n","    d_loss = tf.reduce_mean(0.5 * (d_loss_real + d_loss_fake))\n","    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n","                                                                    labels=tf.ones_like(d_model_fake)))\n","    return d_loss, g_loss"],"metadata":{"_uuid":"8ef5bbb8e4d157577f1b15600aa64cb40289a754","execution":{"iopub.status.busy":"2023-11-03T15:07:00.370885Z","iopub.execute_input":"2023-11-03T15:07:00.371103Z","iopub.status.idle":"2023-11-03T15:07:00.386376Z","shell.execute_reply.started":"2023-11-03T15:07:00.371065Z","shell.execute_reply":"2023-11-03T15:07:00.385558Z"},"trusted":true,"id":"hi98suekzojx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### VAMOS A COMENTAR LAS FUNCIONES QUE HEMOS UTILIZADO PARA CREAR EL OPTIMIZER:\n","\n","**tf.trainable_variables()**: Se utiliza para tener un listado de todas las variables entrenables.\n","\n","**g_vars**: variables relacionadas con el generador\n","\n","**d_vars**: variables relacionadas con el discriminador\n","    \n","**tf.GraphKeys.UPDATE_OPS** es una clave utilizada en TensorFlow para agrupar operaciones en un grafo de cómputo que están relacionadas con la actualización de variables, como las operaciones de normalización o de regularización.\n","    \n","**gen_updates**: Representa las actualizaciones o operaciones que se aplican al generador durante el proceso de entrenamiento de la GAN.\n","\n","**tf.control_dependencies()**: es una función en TensorFlow que se utiliza para controlar el orden de ejecución de las operaciones en un grafo de cómputo. En este caso queremos que se ejecuten primero las del generador.\n","\n","**tf.train.AdamOptimizer()**: El optimizador Adam es un algoritmo de optimización ampliamente utilizado en el aprendizaje profundo para ajustar los pesos y sesgos de un modelo durante el proceso de entrenamiento.\n","\n","    -learning_rate: La tasa de aprendizaje, que controla el tamaño de los pasos que el optimizador toma durante el proceso de entrenamiento.\n","\n","    -beta1: El valor de beta 1 en el algoritmo de momentos de primer orden. Controla la contribución del momento del primer orden (media móvil exponencial del gradiente).\n","    \n","**Variables d_train_opt**: Es la variable refererida al optimizador(en nuestro casos Adam) para entrenar el discriminador. Esto lo hacemos minimizando la variable d_loss que es la perdida del discriminador y con la lista d_vars. Al optimizador le pasamos los siguientes parámetros:\n","    -LR_D = 0.0005\n","    -BETA1_D = 0.5\n","\n","**Variable g_train_opt**: Es la variable refererida al optimizador(en nuestro casos Adam) para entrenar el generador. Esto lo hacemos minimizando la variable g_loss que es la perdida del generador y con la lista dg_vars. Al optimizador le pasamos los siguientes parámetros:\n","    -LR_G = 2e-4\n","    -BETA1_G = 0.5"],"metadata":{"id":"4mLDzxpTzojx"}},{"cell_type":"code","source":["def model_optimizers(d_loss, g_loss):\n","    t_vars = tf.trainable_variables()\n","    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n","    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n","\n","    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","    gen_updates = [op for op in update_ops if op.name.startswith('generator')]\n","\n","    with tf.control_dependencies(gen_updates):\n","        d_train_opt = tf.train.AdamOptimizer(learning_rate=LR_D, beta1=BETA1_D).minimize(d_loss, var_list=d_vars)\n","        g_train_opt = tf.train.AdamOptimizer(learning_rate=LR_G, beta1=BETA1_G).minimize(g_loss, var_list=g_vars)\n","    return d_train_opt, g_train_opt"],"metadata":{"_uuid":"01820b197de1b6d0ca39592043308557d941937a","execution":{"iopub.status.busy":"2023-11-03T15:07:00.389681Z","iopub.execute_input":"2023-11-03T15:07:00.389977Z","iopub.status.idle":"2023-11-03T15:07:00.401998Z","shell.execute_reply.started":"2023-11-03T15:07:00.389934Z","shell.execute_reply":"2023-11-03T15:07:00.401307Z"},"trusted":true,"id":"ZAPh_v8dzojx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **model_inputs()** es una función que te devuelve los inputs del modelo (definiéndolos primero) a partir de la dimensión del vector de ruido y de las dimensiones del data set (lo que posteriormente llamaremos imgsIn)."],"metadata":{"id":"8u9WoNhVzojy"}},{"cell_type":"code","source":["def model_inputs(real_dim, z_dim):\n","    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real')\n","    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n","    learning_rate_G = tf.placeholder(tf.float32, name=\"lr_g\")\n","    learning_rate_D = tf.placeholder(tf.float32, name=\"lr_d\")\n","    return inputs_real, inputs_z, learning_rate_G, learning_rate_D"],"metadata":{"_uuid":"4e8d59264d04344fd0c284c1d41193bd111d4c3f","execution":{"iopub.status.busy":"2023-11-03T15:07:00.404282Z","iopub.execute_input":"2023-11-03T15:07:00.404606Z","iopub.status.idle":"2023-11-03T15:07:00.413506Z","shell.execute_reply.started":"2023-11-03T15:07:00.404545Z","shell.execute_reply":"2023-11-03T15:07:00.412829Z"},"trusted":true,"id":"9ipCOkxFzojy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Otras funciones de utilidad"],"metadata":{"id":"9lgc9H2Pzojy"}},{"cell_type":"markdown","source":["### **Show samples** es una función que se utiliza en el training para ir mostrando la evolución del mismo. Sin emabargo, solo lo utilizamos cuando testeamos la red, ya que cuando la entrenamos evitamos usarla para mejorar los tiempos de ejecución."],"metadata":{"id":"8EZOMCA0zojy"}},{"cell_type":"code","source":["def show_samples(sample_images, name, epoch):\n","    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n","    for index, axis in enumerate(axes):\n","        axis.axis('off')\n","        image_array = sample_images[index].astype('uint8')\n","        axis.imshow(image_array)\n","    plt.show()\n","    plt.close()"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T15:07:00.414734Z","iopub.execute_input":"2023-11-03T15:07:00.415039Z","iopub.status.idle":"2023-11-03T15:07:00.428990Z","shell.execute_reply.started":"2023-11-03T15:07:00.414981Z","shell.execute_reply":"2023-11-03T15:07:00.428359Z"},"trusted":true,"id":"uaFA4ZKyzojz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **test** se utiliza para generar las imágenes de muestra que se muestran con **show_samples**, por lo que tampoco la vamos a utilizar."],"metadata":{"id":"vYcGjVJyzojz"}},{"cell_type":"code","source":["def test(sess, input_z, out_channel_dim, epoch):\n","    example_z = np.random.uniform(-1, 1, size=[SAMPLES_TO_SHOW, input_z.get_shape().as_list()[-1]])\n","    samples = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\n","    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n","    #show_samples(sample_images, IMG_DIR + \"samples\", epoch)"],"metadata":{"_uuid":"43677a2f90b65e39110c1f547a928bb1b5f9d892","execution":{"iopub.status.busy":"2023-11-03T15:07:00.442394Z","iopub.execute_input":"2023-11-03T15:07:00.442703Z","iopub.status.idle":"2023-11-03T15:07:00.452530Z","shell.execute_reply.started":"2023-11-03T15:07:00.442652Z","shell.execute_reply":"2023-11-03T15:07:00.451841Z"},"trusted":true,"id":"AZsMFD1Dzojz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **generate** es la función utilizada para la generación de los datos una vez la GAN ha sido entrenada. Toma como inputs la sesión de tf, los vectores de ruido z y la dimensión del canal de salida RGB (3).\n","\n","Genera las imágenes utilizando el generador ya entrenado y los guarda en una lista, que luego convertiremos en un vector de numpy."],"metadata":{"id":"Tp-HstbUzojz"}},{"cell_type":"code","source":["def generate (sess, input_z, out_channel_dim):\n","    print (\"Generando\")\n","    imgs2=[]\n","    for i in tqdm(range(10)):\n","        example_z = np.random.uniform(-1, 1, size=[100, 100]).astype(np.float32)\n","        imgs = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\n","        imgs = [((img + 1.0) * 127.5).astype(np.uint8) for img in imgs]\n","        for j in range(len(imgs)):\n","            imgs2.append(imgs[j])\n","    return imgs2"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T15:07:00.453575Z","iopub.execute_input":"2023-11-03T15:07:00.453834Z","iopub.status.idle":"2023-11-03T15:07:00.465077Z","shell.execute_reply.started":"2023-11-03T15:07:00.453783Z","shell.execute_reply":"2023-11-03T15:07:00.464475Z"},"trusted":true,"id":"l8m9GhI2zojz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def summarize_epoch(epoch, duration, sess, d_losses, g_losses, input_z, data_shape):\n","    minibatch_size = int(data_shape[0]//BATCH_SIZE)\n","    '''\n","    print(\"Epoch {}/{}\".format(epoch, EPOCHS),\n","          \"\\nDuration: {:.5f}\".format(duration),\n","          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n","          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n","    '''\n","    #fig, ax = plt.subplots()\n","    #plt.plot(d_losses, label='Discriminator', alpha=0.6)\n","    #plt.plot(g_losses, label='Generator', alpha=0.6)\n","    #plt.title(\"Losses\")\n","    #plt.legend()\n","    #plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n","    #plt.show()\n","    #plt.close()\n","    test(sess, input_z, data_shape[3], epoch)"],"metadata":{"_uuid":"0ed8f1c378f936ac81ae89b35d5b6914cd6efbbb","execution":{"iopub.status.busy":"2023-11-03T15:07:00.466398Z","iopub.execute_input":"2023-11-03T15:07:00.466717Z","iopub.status.idle":"2023-11-03T15:07:00.476428Z","shell.execute_reply.started":"2023-11-03T15:07:00.466662Z","shell.execute_reply":"2023-11-03T15:07:00.475780Z"},"trusted":true,"id":"RL2zcky0zoj0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **get_batches** es una función que, que como su nombre indica, toma los datos y genera los batches y los normaliza."],"metadata":{"id":"OakSAtK3zoj1"}},{"cell_type":"code","source":["def get_batches(data):\n","    batches = []\n","    for i in range(int(data.shape[0]//BATCH_SIZE)):\n","        batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n","        augmented_images = []\n","        for img in batch:\n","            image = Image.fromarray(img.astype('uint8'))\n","            if random.choice([True, False]):\n","                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n","            augmented_images.append(np.asarray(image))\n","        batch = np.asarray(augmented_images)\n","        normalized_batch = (batch / 127.5) - 1.0\n","        batches.append(normalized_batch)\n","    return batches"],"metadata":{"_uuid":"5977b8035723edd4a013406cb5376848d62dcc0a","execution":{"iopub.status.busy":"2023-11-03T15:07:00.478027Z","iopub.execute_input":"2023-11-03T15:07:00.478345Z","iopub.status.idle":"2023-11-03T15:07:00.491146Z","shell.execute_reply.started":"2023-11-03T15:07:00.478290Z","shell.execute_reply":"2023-11-03T15:07:00.490556Z"},"trusted":true,"id":"MPLJKWz8zoj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entrenamiento"],"metadata":{"id":"fQ0GCtn1zoj1"}},{"cell_type":"markdown","source":["### La función **train** toma como argumentos de entrada los **batches** con la función **get_bataches**, que trocea nuestro dataset, las dimensiones de dataset, el learning rate del generador (LR_G) y del discriminador (LR_D), que hemos fijado con valores muy bajos para poder captar las características del dataset de manera más rica. La estructura de la función train es la siguiente:\n","\n","  - En primer lugar generamos los inputs del modelo con **model_inputs** y la función de pérdida y el optimizador con **model_loss** y **model_optimizers**.\n","\n","  - Inicializamos la sesión de tensor flow, las épocas y las iteraciones, para a continuación movernos entre las épocas y los lotes para computar las funcciones de pérdida y actualizar los valores de los parámetros una vez se ha terminado el batch,.\n","\n","  - Por último, una vez se hayan ejecutado todas las épocas, la GAN ya está esntrenada, por lo que simplemente llamamos a la función **generate** para que nos genere las imágenes que precisemos. Hemos ajustado los valores para que genere exactamente 1000 imágenes de cada tipo, aunque podríamos haberlo generado de manera arbitraria."],"metadata":{"id":"pXTSTYtMzoj1"}},{"cell_type":"code","source":["def train(get_batches, data_shape, LR_G = 2e-4, LR_D = 0.0005):\n","    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], NOISE_SIZE)\n","    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3])\n","    d_opt, g_opt = model_optimizers(d_loss, g_loss)\n","    generator_epoch_loss = 0\n","    train_d_losses = []\n","    train_g_losses = []\n","    generator_epoch_loss = 999\n","\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        epoch = 0\n","        iteration = 0\n","        d_losses = []\n","        g_losses = []\n","\n","        for epoch in tqdm(range(EPOCHS)):\n","            epoch += 1\n","            start_time = time.time()\n","\n","            for batch_images in get_batches:\n","                iteration += 1\n","                batch_z = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))\n","                _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: LR_D})\n","                _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: LR_G})\n","                d_losses.append(d_loss.eval({input_z: batch_z, input_images: batch_images}))\n","                g_losses.append(g_loss.eval({input_z: batch_z}))\n","\n","            summarize_epoch(epoch, time.time()-start_time, sess, d_losses, g_losses, input_z, data_shape)\n","            minibatch_size = int(data_shape[0]//BATCH_SIZE)\n","            generator_epoch_loss = np.mean(g_losses[-minibatch_size:])\n","            train_d_losses.append(np.mean(d_losses[-minibatch_size:]))\n","            train_g_losses.append(np.mean(g_losses[-minibatch_size:]))\n","\n","            if epoch == EPOCHS:\n","                imgs=generate (sess, input_z, out_channel_dim=3)\n","\n","    #fig, ax = plt.subplots()\n","    #plt.plot(train_d_losses, label='Discriminator', alpha=0.5)\n","    #plt.plot(train_g_losses, label='Generator', alpha=0.5)\n","    #plt.title(\"Training Losses\")\n","    #plt.legend()\n","    #plt.savefig('train_losses.png')\n","    #plt.show()\n","    #plt.close()\n","    return imgs"],"metadata":{"_uuid":"578f639e8a70bed5c797f0f4655d318f1a07f6fc","execution":{"iopub.status.busy":"2023-11-03T15:07:00.492407Z","iopub.execute_input":"2023-11-03T15:07:00.492752Z","iopub.status.idle":"2023-11-03T15:07:00.509120Z","shell.execute_reply.started":"2023-11-03T15:07:00.492696Z","shell.execute_reply":"2023-11-03T15:07:00.508564Z"},"trusted":true,"id":"L_Ss-aeizoj2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Inicialización de los hiperparámetros**"],"metadata":{"id":"ABVhe5yJzoj2"}},{"cell_type":"code","source":["IMAGE_SIZE = 64\n","NOISE_SIZE = 100\n","LR_D = 0.0005\n","LR_G = 2e-4\n","BATCH_SIZE = 64\n","EPOCHS = 100\n","BETA1_G = 0.5\n","BETA1_D = 0.5\n","WEIGHT_INIT_STDDEV = 0.02\n","MOMENTUM = 0.9\n","EPSILON = 0.0005\n","SAMPLES_TO_SHOW = 5 # each epoch"],"metadata":{"_uuid":"86335745ed2e6b406839c5d85a1085aab276e73a","execution":{"iopub.status.busy":"2023-11-03T15:07:00.510103Z","iopub.execute_input":"2023-11-03T15:07:00.510340Z","iopub.status.idle":"2023-11-03T15:07:00.523693Z","shell.execute_reply.started":"2023-11-03T15:07:00.510305Z","shell.execute_reply":"2023-11-03T15:07:00.523048Z"},"trusted":true,"id":"iZP8vgd3zoj2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PROCESAMIENTO DE LOS DATOS, ENTRENAMIENTO DE LA RED Y GENERACIÓN DE LOS NUEVOS DATOS SINTÉTICOS"],"metadata":{"id":"fBLID-cizoj3"}},{"cell_type":"markdown","source":["**Librerías necesarias**"],"metadata":{"id":"Ze9cH-npzoj3"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.preprocessing import OneHotEncoder\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","import numpy as np\n","import albumentations as A\n","import time\n","from tqdm import tqdm"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T15:07:00.524883Z","iopub.execute_input":"2023-11-03T15:07:00.525134Z","iopub.status.idle":"2023-11-03T15:07:03.698814Z","shell.execute_reply.started":"2023-11-03T15:07:00.525086Z","shell.execute_reply":"2023-11-03T15:07:03.698031Z"},"trusted":true,"id":"BG6PyVJXzoj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Recordamos el mapeo que habíamos hecho sobre las etiquetas, aunque no lo vayamos a utilizar."],"metadata":{"id":"OfdzdNKIzoj4"}},{"cell_type":"code","source":["#RECORDAMOS EL DICCIONARIO CON EL 'mapeo' DE LOS PARÁMETROS\n","map_labels = {\n","    'Apple Braeburn':0,\n","  'Apple Granny Smith':1,\n","  'Apricot':2,\n","  'Avocado':3,\n","  'Banana':4,\n","  'Blueberry':5,\n","  'Cactus fruit':6,\n","  'Cantaloupe':7,\n","  'Cherry':8,\n","  'Clementine':9,\n","  'Corn':10,\n","  'Cucumber Ripe':11,\n","  'Grape Blue':12,\n","  'Kiwi':13,\n","  'Lemon':14,\n","  'Limes':15,\n","  'Mango':16,\n","  'Onion White':17,\n","  'Orange':18,\n","  'Papaya':19,\n","  'Passion Fruit':20,\n","  'Peach':21,\n","  'Pear':22,\n","  'Pepper Green':23,\n","  'Pepper Red':24,\n","  'Pineapple':25,\n","  'Plum':26,\n","  'Pomegranate':27,\n","  'Potato Red':28,\n","  'Raspberry':29,\n","  'Strawberry':30,\n","  'Tomato':31,\n","  'Watermelon':32\n","}"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T15:07:03.700354Z","iopub.execute_input":"2023-11-03T15:07:03.700699Z","iopub.status.idle":"2023-11-03T15:07:03.709469Z","shell.execute_reply.started":"2023-11-03T15:07:03.700637Z","shell.execute_reply":"2023-11-03T15:07:03.708658Z"},"trusted":true,"id":"PPKqkRqSzoj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cargamos los datos"],"metadata":{"id":"YQBXco2Azoj4"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os, warnings\n","\n","#VAMOS A IMPORTAR NUESTROS DATOS\n","file_path = '/kaggle/input/data-sergio-npz/data_tres_transformaciones.npz'\n","\n","\n","with np.load(file_path) as data:\n","    # Acceder a los datos del archivo .npz por nombre\n","    trainX = data['data']\n","    trainY = data['labels'] # cargar el array con los valores de las etiqeutas\n","\n","# Trasnformar en arrays de numpy para mayor eficiencia\n","trainX = np.array(trainX)\n","trainY = np.array(trainY)\n","# Mostrar dimension del conjunto de muestras total\n","print(\"Forma de vector trainX de muestras:\", trainX.shape)\n","print(\"Forma de vector trainY de etiquetas:\", trainY.shape)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T15:07:03.710818Z","iopub.execute_input":"2023-11-03T15:07:03.711124Z","iopub.status.idle":"2023-11-03T15:07:12.905695Z","shell.execute_reply.started":"2023-11-03T15:07:03.711058Z","shell.execute_reply":"2023-11-03T15:07:12.904940Z"},"trusted":true,"id":"YKFDzHP3zoj5","outputId":"ff2f8c63-1cc7-44ab-f1d2-6b0a16612d2c"},"execution_count":null,"outputs":[{"name":"stdout","text":"Forma de vector trainX de muestras: (33000, 100, 100, 3)\nForma de vector trainY de etiquetas: (33000,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Definimos algunas funciones útiles que vamos a necesitar para el procesamiento de los datos"],"metadata":{"id":"dcBYm6uyzoj-"}},{"cell_type":"code","source":["#VAMOS A DEFINIR ALGUNAS FUNCIONES QUE NOS VAN A SER UTILES EN LA GENERACION DE ESTAS NUEVAS IMAGENES\n","def extraer (num_label,trainX,trainY):\n","  lista=[]\n","  for i in range (0,len(trainX)):\n","    if trainY[i]==num_label:\n","      lista.append(trainX[i])\n","  return lista\n","\n","\n","#REESCALAMOS PARA QUE NOS QUEDE 64X64:\n","def reescalado_64(trainX):\n","  nuevo_trainX=[]\n","  for i in range (0,len(trainX)):\n","    imagen=trainX[i]\n","    canal_r = cv2.resize(imagen[:,:,0], (64, 64))\n","    canal_g = cv2.resize(imagen[:,:,1], (64, 64))\n","    canal_b = cv2.resize(imagen[:,:,2], (64, 64))\n","    nuevo_trainX.append(list(np.stack((canal_r, canal_g, canal_b), axis=-1)))\n","  return nuevo_trainX\n","\n","#UN REESCALADO PARA QUE TODOS ESTÉN ENTRE -1 Y 1, AL FINAL NO HA SIDO UTILIZADO\n","def normalizacion(images):\n","  images_rescaled=(images-np.min(images)*np.ones_like(images))/(255-np.min(images))\n","  return images_rescaled"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T15:07:12.907148Z","iopub.execute_input":"2023-11-03T15:07:12.907493Z","iopub.status.idle":"2023-11-03T15:07:12.919258Z","shell.execute_reply.started":"2023-11-03T15:07:12.907406Z","shell.execute_reply":"2023-11-03T15:07:12.918616Z"},"trusted":true,"id":"bpsmn6Vyzoj_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creamos una función que, dado el número asociado a una etiqueta y los datos, me entrena a la GAN y me genera tantos datos como quiera, como ya hemos mencionado serán unos 1000 por etiqueta."],"metadata":{"id":"GWgMnzdnzoj_"}},{"cell_type":"code","source":["def generator_dcgan(num_label,trainX,trainY):\n","  #HACEMOS LA EXTRACCIÓN DE LOS DATOS Y SU REESCALADO INICIAL\n","  imagesIn=np.array(extraer(num_label,trainX,trainY))\n","  imagesIn=np.array(reescalado_64(imagesIn))\n","  imagesIn.shape\n","  #AQUI EL BUCLE DE ENTRENAMIENTO:\n","  start = time.time()\n","\n","  print(\">> Start training...\")\n","  with tf.Graph().as_default():\n","    imgs=train(get_batches(imagesIn), imagesIn.shape)\n","  print(\">> train time = \",time.time() - start)\n","  #TIENE QUE SER UNA LISTA, PARA PODER RESHAPEARLO. SE SUPONE QUE LO ES, PERO POR SI ACASO\n","  imgs=list(imgs)\n","  #HACEMOS EL RESHAPE:\n","  for i in range (len(imgs)):\n","    imgs[i]= cv2.resize(imgs[i], (100, 100))\n","  imgs=np.array(imgs)\n","\n","  return imgs\n","\n"],"metadata":{"_kg_hide-input":true,"_uuid":"1ee6349afccb4d2f29f36d6605dd2f156350821a","execution":{"iopub.status.busy":"2023-11-03T15:07:12.920423Z","iopub.execute_input":"2023-11-03T15:07:12.920668Z","iopub.status.idle":"2023-11-03T15:07:12.935918Z","shell.execute_reply.started":"2023-11-03T15:07:12.920629Z","shell.execute_reply":"2023-11-03T15:07:12.935262Z"},"trusted":true,"id":"IOcL1ByCzoj_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creamos el bucle que nos genere los datos para todas las etiquetas"],"metadata":{"id":"VFfC8Dj5zoj_"}},{"cell_type":"code","source":["#VAMOS A SACAR TODAS LAS IMÁGENES PARA TODAS LAS FRUTAS\n","frutas_generadas=[]\n","etiquetas=[]\n","n_class=33\n","for num_label in range (0,n_class):\n","    etiquetas=np.array(etiquetas)\n","    print(f'Vamos por la iteración {num_label}')\n","    frutas_generadas.append(generator_dcgan(num_label,trainX,trainY))\n","    nueva_etiqueta=np.ones(len(frutas_generadas[num_label]))*num_label\n","    etiquetas=np.concatenate([etiquetas,nueva_etiqueta])\n","print('Generación finalizada')"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T15:07:12.937115Z","iopub.execute_input":"2023-11-03T15:07:12.937326Z","iopub.status.idle":"2023-11-03T17:17:58.300139Z","shell.execute_reply.started":"2023-11-03T15:07:12.937289Z","shell.execute_reply":"2023-11-03T17:17:58.299304Z"},"trusted":true,"id":"udvi1eQZzoj_","outputId":"35fa47c2-4a6b-4fef-c973-c5c2e3e4a23e"},"execution_count":null,"outputs":[{"name":"stdout","text":"Vamos por la iteración 0\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.50s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.05it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.06it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.07it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.08it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.06it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.04it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.04it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.04it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.03it/s]\u001b[A\n100%|██████████| 100/100 [03:52<00:00,  5.37s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  239.04094648361206\nVamos por la iteración 1\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.53s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.12it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.11it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.07it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.07it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.06it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.05it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.04it/s]\u001b[A\n100%|██████████| 100/100 [03:52<00:00,  5.37s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  235.31627082824707\nVamos por la iteración 2\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.50s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.10it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.10it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.06it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.05it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:04,  1.03s/it]\u001b[A\n 70%|███████   | 7/10 [00:06<00:03,  1.01s/it]\u001b[A\n 80%|████████  | 8/10 [00:07<00:02,  1.01s/it]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.00it/s]\u001b[A\n100%|██████████| 100/100 [03:53<00:00,  5.47s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  235.6243658065796\nVamos por la iteración 3\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.57s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.10it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.09it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.08it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.07it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.07it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.05it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.05it/s]\u001b[A\n100%|██████████| 100/100 [03:52<00:00,  5.39s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  235.38817739486694\nVamos por la iteración 4\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.50s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.08it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.08it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.05it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.04it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.04it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.04it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.04it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.05it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.05it/s]\u001b[A\n100%|██████████| 100/100 [03:52<00:00,  5.38s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  234.9685275554657\nVamos por la iteración 5\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:37<00:02,  2.51s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:07,  1.13it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.12it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.12it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.10it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.10it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.09it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.08it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.08it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.07it/s]\u001b[A\n100%|██████████| 100/100 [03:48<00:00,  5.29s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  232.07906103134155\nVamos por la iteración 6\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:39<00:02,  2.49s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.09it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.09it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.08it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:04,  1.02s/it]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.01it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.02it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.03it/s]\u001b[A\n100%|██████████| 100/100 [03:51<00:00,  5.42s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  234.49126601219177\nVamos por la iteración 7\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.56s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.11it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.10it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.07it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.07it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.07it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.06it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.04it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.02it/s]\u001b[A\n100%|██████████| 100/100 [03:52<00:00,  5.43s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  235.28647351264954\nVamos por la iteración 8\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.48s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.12it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.12it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.11it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.07it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.06it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.06it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.06it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.05it/s]\u001b[A\n100%|██████████| 100/100 [03:51<00:00,  5.32s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  234.57366037368774\nVamos por la iteración 9\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:41<00:02,  2.60s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.06it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.05it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.04it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.04it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.03it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.03it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.03it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.01it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.00it/s]\u001b[A\n100%|██████████| 100/100 [03:53<00:00,  5.53s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  236.63953733444214\nVamos por la iteración 10\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:41<00:02,  2.48s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.09it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.09it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.08it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:04,  1.02s/it]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.00it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.02it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.03it/s]\u001b[A\n100%|██████████| 100/100 [03:54<00:00,  5.41s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  236.68722534179688\nVamos por la iteración 11\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.60s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.04it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.03it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.02it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.01it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.02it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.03it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.02it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.01it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.01it/s]\u001b[A\n100%|██████████| 100/100 [03:53<00:00,  5.58s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  235.85917711257935\nVamos por la iteración 12\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:41<00:02,  2.54s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.05it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.04it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.03it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.03it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.02it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.02it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.01it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.01it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.01it/s]\u001b[A\n100%|██████████| 100/100 [03:54<00:00,  5.50s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  236.76919078826904\nVamos por la iteración 13\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:38<00:02,  2.52s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.12it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.12it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.08it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.08it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.08it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.07it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.06it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.05it/s]\u001b[A\n100%|██████████| 100/100 [03:50<00:00,  5.34s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  233.8412172794342\nVamos por la iteración 14\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:39<00:02,  2.50s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.10it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.09it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.07it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.07it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.07it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:04,  1.01s/it]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.01it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.02it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.01it/s]\u001b[A\n100%|██████████| 100/100 [03:52<00:00,  5.43s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  234.70623445510864\nVamos por la iteración 15\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:40<00:02,  2.57s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.10it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.10it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.08it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.08it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.07it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.05it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.04it/s]\u001b[A\n100%|██████████| 100/100 [03:52<00:00,  5.39s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  234.93402910232544\nVamos por la iteración 16\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:36<00:02,  2.44s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.05it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.05it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.04it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.04it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.05it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.04it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.05it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.02it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.01it/s]\u001b[A\n100%|██████████| 100/100 [03:49<00:00,  5.42s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  231.89326667785645\nVamos por la iteración 17\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:35<00:02,  2.52s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:07,  1.13it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.13it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.12it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.12it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.10it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.10it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.10it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.09it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.09it/s]\u001b[A\n100%|██████████| 100/100 [03:47<00:00,  5.26s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  230.43210005760193\nVamos por la iteración 18\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:36<00:02,  2.43s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:07,  1.15it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:06,  1.15it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.14it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.12it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.12it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.03it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.01it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.03it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.03it/s]\u001b[A\n100%|██████████| 100/100 [03:48<00:00,  5.28s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  231.16069197654724\nVamos por la iteración 19\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:38<00:02,  2.54s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.10it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.10it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.10it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.09it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.09it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.08it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.08it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.07it/s]\u001b[A\n100%|██████████| 100/100 [03:49<00:00,  5.31s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  232.57256054878235\nVamos por la iteración 20\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:38<00:02,  2.47s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:07,  1.13it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.11it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.11it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.11it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.11it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.10it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.09it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.07it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.07it/s]\u001b[A\n100%|██████████| 100/100 [03:50<00:00,  5.26s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  232.65316820144653\nVamos por la iteración 21\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:34<00:02,  2.47s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:07,  1.17it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:06,  1.16it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.16it/s]\u001b[A\n 40%|████      | 4/10 [00:03<00:05,  1.14it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.14it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.14it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.13it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.12it/s]\u001b[A\n 90%|█████████ | 9/10 [00:08<00:00,  1.10it/s]\u001b[A\n100%|██████████| 100/100 [03:45<00:00,  5.19s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  228.99735808372498\nVamos por la iteración 22\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:42<00:02,  2.66s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.07s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.06s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.06s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.06s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.06s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.16s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.14s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.13s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.12s/it]\u001b[A\n100%|██████████| 100/100 [03:56<00:00,  5.99s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  239.16527247428894\nVamos por la iteración 23\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:50<00:02,  2.70s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.05s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.06s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.05s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.06s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.07s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.09s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.08s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.09s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.10s/it]\u001b[A\n100%|██████████| 100/100 [04:03<00:00,  5.95s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  246.40610241889954\nVamos por la iteración 24\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:49<00:02,  2.62s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.04s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.03s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.03s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.04s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.05s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.06s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.06s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.07s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.07s/it]\u001b[A\n100%|██████████| 100/100 [04:02<00:00,  5.82s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  245.35224962234497\nVamos por la iteración 25\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:42<00:02,  2.54s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.01it/s]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.02s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:06,  1.01it/s]\u001b[A\n 40%|████      | 4/10 [00:04<00:05,  1.01it/s]\u001b[A\n 50%|█████     | 5/10 [00:04<00:04,  1.02it/s]\u001b[A\n 60%|██████    | 6/10 [00:05<00:03,  1.00it/s]\u001b[A\n 70%|███████   | 7/10 [00:06<00:02,  1.00it/s]\u001b[A\n 80%|████████  | 8/10 [00:07<00:01,  1.00it/s]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.01s/it]\u001b[A\n100%|██████████| 100/100 [03:55<00:00,  5.55s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  238.3450276851654\nVamos por la iteración 26\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:47<00:02,  2.61s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.02it/s]\u001b[A\n 20%|██        | 2/10 [00:02<00:07,  1.01it/s]\u001b[A\n 30%|███       | 3/10 [00:03<00:06,  1.00it/s]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.01s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.03s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.13s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.12s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.10s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.11s/it]\u001b[A\n100%|██████████| 100/100 [04:01<00:00,  5.89s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  243.89071559906006\nVamos por la iteración 27\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:49<00:02,  2.67s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:00<00:08,  1.00it/s]\u001b[A\n 20%|██        | 2/10 [00:01<00:07,  1.00it/s]\u001b[A\n 30%|███       | 3/10 [00:02<00:06,  1.01it/s]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.01s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.02s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.04s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.04s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.04s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.06s/it]\u001b[A\n100%|██████████| 100/100 [04:02<00:00,  5.79s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  245.19018411636353\nVamos por la iteración 28\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:49<00:02,  2.67s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.04s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.05s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.06s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.07s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.06s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.07s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.07s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.07s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.08s/it]\u001b[A\n100%|██████████| 100/100 [04:03<00:00,  5.90s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  246.10990452766418\nVamos por la iteración 29\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:47<00:02,  2.66s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.03s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.02s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.02s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.02s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.04s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.05s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.06s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.08s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.08s/it]\u001b[A\n100%|██████████| 100/100 [04:01<00:00,  5.84s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  244.51723718643188\nVamos por la iteración 30\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:47<00:02,  2.62s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.05s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.04s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.04s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.05s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.04s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.12s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.11s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.10s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.11s/it]\u001b[A\n100%|██████████| 100/100 [04:01<00:00,  5.90s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  244.21963000297546\nVamos por la iteración 31\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:49<00:02,  2.69s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.02s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.02s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.03s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.05s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.06s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.07s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.09s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.10s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.11s/it]\u001b[A\n100%|██████████| 100/100 [04:02<00:00,  5.93s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  245.4386088848114\nVamos por la iteración 32\n>> Start training...\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 99/100 [03:48<00:02,  2.61s/it]\n  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Generando\n","output_type":"stream"},{"name":"stderr","text":"\n 10%|█         | 1/10 [00:01<00:09,  1.02s/it]\u001b[A\n 20%|██        | 2/10 [00:02<00:08,  1.02s/it]\u001b[A\n 30%|███       | 3/10 [00:03<00:07,  1.03s/it]\u001b[A\n 40%|████      | 4/10 [00:04<00:06,  1.05s/it]\u001b[A\n 50%|█████     | 5/10 [00:05<00:05,  1.05s/it]\u001b[A\n 60%|██████    | 6/10 [00:06<00:04,  1.06s/it]\u001b[A\n 70%|███████   | 7/10 [00:07<00:03,  1.08s/it]\u001b[A\n 80%|████████  | 8/10 [00:08<00:02,  1.09s/it]\u001b[A\n 90%|█████████ | 9/10 [00:09<00:01,  1.11s/it]\u001b[A\n100%|██████████| 100/100 [04:01<00:00,  5.86s/it]A\n","output_type":"stream"},{"name":"stdout","text":">> train time =  244.34806323051453\nGeneración finalizada\n","output_type":"stream"}]},{"cell_type":"markdown","source":["### Guardamos los datos en un archivo de extensión .npz para poder exportarlos y evaluarlos en el clasificador."],"metadata":{"id":"Owr-Vm9UzokA"}},{"cell_type":"code","source":["#LAS CONVERTIMOS EN ARRAYS NUEVAMENTE\n","frutas_generadas=np.array(frutas_generadas)\n","etiquetas=np.array(etiquetas)\n","frutas_final=[]\n","for i in range (len(frutas_generadas)):\n","    for j in range (0,len(frutas_generadas[i])):\n","        frutas_final.append(frutas_generadas[i,j])\n","frutas_final=np.array(frutas_final)\n","print(frutas_final.shape,etiquetas.shape,trainX.shape)\n","\n","#LOS JUNTAMOS CON LAS QUE NOS HAN DADO DE INICIO\n","array_final_datos=np.concatenate([trainX,frutas_final])\n","array_final_etiquetas=np.concatenate([trainY,etiquetas])\n","\n","#LO EXPORTAMOS TODO COMO UN UNICO ARCHIVO .npz, QUE TIENE LA MISMA ESTRUCTURA QUE EL ARCHIVO DE ENTRADA\n","np.savez('datos_gan',data=array_final_datos,labels=array_final_etiquetas)"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T17:17:58.301735Z","iopub.execute_input":"2023-11-03T17:17:58.302002Z","iopub.status.idle":"2023-11-03T17:18:04.722874Z","shell.execute_reply.started":"2023-11-03T17:17:58.301949Z","shell.execute_reply":"2023-11-03T17:18:04.721965Z"},"trusted":true,"id":"d4V7pIUHzokA","outputId":"7de41900-219e-4665-f2bf-d4a6deb4d22f"},"execution_count":null,"outputs":[{"name":"stdout","text":"(33000, 100, 100, 3) (33000,) (33000, 100, 100, 3)\n","output_type":"stream"}]}]}